{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23311361-844a-48e4-9d9c-89d3de3cfc18",
   "metadata": {},
   "source": [
    "# Monster Tracking\n",
    "\n",
    "First import relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea113bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b99f0cd-7109-41dd-99b9-fbc562e72a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafka\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, rand\n",
    "from geopy.geocoders import Nominatim\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from geonamescache import GeonamesCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44455da1-c4e3-475c-98d7-a14cf5955162",
   "metadata": {},
   "source": [
    " Create a topic called: monster_movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade3b72-a4ea-4193-a31f-c46804a0e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client = KafkaAdminClient(bootstrap_servers=\"kafka:9092\")\n",
    "topics = [NewTopic(name=\"monster_movement\", num_partitions=1, replication_factor=1)]\n",
    "admin_client.create_topics(new_topics=topics)\n",
    "admin_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e74628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin_client = KafkaAdminClient(bootstrap_servers=\"kafka:9092\")\n",
    "# topics = [NewTopic(name=\"country_damage\", num_partitions=1, replication_factor=1)]\n",
    "# admin_client.create_topics(new_topics=topics)\n",
    "# admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89dc5d6-7deb-483a-beb2-15c44d40376a",
   "metadata": {},
   "source": [
    "Next we'll generate the data. We want to use the functions we create in our Spark streaming practice examples. What we want to achieve is:\n",
    "\n",
    "- Create 200 events.\n",
    "- The event should pick five random rows from our dnd_monsters.csv and return the monster's name and str characteristic.\n",
    "- Along with the name and str we need the latitude and longitude of the monster and a timestamp. Latitude is between -90 and 90 and longitude is -180 to 180.\n",
    "- Add a timestamp field called ts.\n",
    "- Remember to put everything into JSON then into a field called value for the kafka message.\n",
    "- Don't write to Kafka yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d6d139-5a6c-4a6d-8776-dcd256c7feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                name| str|\n",
      "+--------------------+----+\n",
      "|           aarakocra|10.0|\n",
      "|             abjurer|null|\n",
      "|             aboleth|21.0|\n",
      "|     abominable-yeti|null|\n",
      "|            acererak|null|\n",
      "|             acolyte|10.0|\n",
      "|  adult-black-dragon|23.0|\n",
      "|adult-blue-dracolich|null|\n",
      "|   adult-blue-dragon|25.0|\n",
      "|  adult-brass-dragon|23.0|\n",
      "| adult-bronze-dragon|25.0|\n",
      "| adult-copper-dragon|23.0|\n",
      "|   adult-gold-dragon|27.0|\n",
      "|  adult-green-dragon|23.0|\n",
      "|       adult-kruthik|null|\n",
      "|         adult-oblex|null|\n",
      "|    adult-red-dragon|27.0|\n",
      "| adult-silver-dragon|27.0|\n",
      "|  adult-white-dragon|22.0|\n",
      "|       air-elemental|14.0|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession and context\n",
    "spark = SparkSession.builder.appName(\"MonsterTracker\").config(\"spark.jars\",\"work/data/commons-pool2-2.11.1.jar,work/data/spark-sql-kafka-0-10_2.12-3.4.0.jar,work/data/spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar\").getOrCreate()\n",
    "\n",
    "# Load data into dataframe\n",
    "path = \"./work/data/dnd_monsters.csv\"\n",
    "df = spark.read.options(header=True, inferSchema=True).csv(path)\n",
    "#df.show()\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"monsters\")\n",
    "\n",
    "# Perform SQL query to get name and strength on the DataFrame\n",
    "sql_query = \"\"\"\n",
    "    SELECT name, str\n",
    "    FROM monsters\n",
    "\"\"\"\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Show the query result\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e9b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m rawQuery \u001b[39m=\u001b[39m stream_spark_df\u001b[39m.\u001b[39mwriteStream\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mconsole\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moutputMode(\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     16\u001b[0m \u001b[39m# Once you start the stream you either end it or keep it listening for more data with\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# the awaitTermination option\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m rawQuery\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating a SparkSession with the extra jars we need for Kafka\n",
    "stream_spark = SparkSession.builder.appName(\"Monster_Stream\").config(\"spark.jars\",\"work/data/commons-pool2-2.11.1.jar,work/data/spark-sql-kafka-0-10_2.12-3.4.0.jar,work/data/spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar\").getOrCreate()\n",
    "\n",
    "# Read Kafka as a stream, you can read it as a batch also.  Simple read everything\n",
    "# on the topic into one DataFrame going back to the beginning\n",
    "stream_spark_df = stream_spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9092\").option(\"subscribe\", \"monster_movement\").option(\"startingOffsets\",\"earliest\").load()\n",
    "\n",
    "# Commented out code which would make the output more readable\n",
    "# df = df.selectExpr(\"CAST(key AS STRING) as Key\", \"CAST(value AS STRING) as Value\")\n",
    "\n",
    "stream_spark_df = stream_spark_df.selectExpr(\"CAST(value AS STRING) as Value\")\n",
    "# Here we take the previous stream from Kafka and write that stream to the console \n",
    "rawQuery = stream_spark_df.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "\n",
    "\n",
    "# Once you start the stream you either end it or keep it listening for more data with\n",
    "# the awaitTermination option\n",
    "rawQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4dd47d2-a3ba-45bd-b0b4-af1627794f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event  1\n",
      "Event  2\n",
      "Event  3\n",
      "Event  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0xffff3d6428d0>, 'Connection to nominatim.openstreetmap.org timed out. (connect timeout=1)')': /reverse?lat=27.0&lon=-2.0&format=json&accept-language=en&addressdetails=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event  5\n",
      "Event  6\n",
      "Event  7\n",
      "Event  8\n",
      "Event  9\n"
     ]
    }
   ],
   "source": [
    "def country_finder(latitude, longitude): \n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder\") # Create a geocoder instance\n",
    "    location = geolocator.reverse((latitude, longitude), language='en') # Reverse geocode to get the location information\n",
    "    try:\n",
    "        country = location.raw['address']['country']\n",
    "    except:\n",
    "        return \"sea\"\n",
    "    return country\n",
    "\n",
    "def get_country_population(country_name):\n",
    "    geonames = GeonamesCache()\n",
    "    # population = 1\n",
    "    try:\n",
    "        country_data = geonames.get_countries_by_names().get(country_name)\n",
    "        population = country_data.get('population')\n",
    "    except:\n",
    "        population = 1\n",
    "        print(\"{} country not found\".format(population))\n",
    "    return population\n",
    "\n",
    "for int in range(1,10):\n",
    "    print('Event ', int)\n",
    "    monster_json_string = ''\n",
    "    random_lines = result.orderBy(rand()).limit(5)\n",
    "    list_tuples = random_lines.rdd.map(tuple).collect()\n",
    "    for monster in list_tuples:\n",
    "        lat = random.randint(-90, 90)\n",
    "        long = random.randint(-180, 180)\n",
    "        country = country_finder(lat, long)\n",
    "        if country != 'sea':\n",
    "            population = get_country_population(country)\n",
    "            if (population > 1) & (type(monster[1]) == int):\n",
    "                damage = population * monster[1] / 1000\n",
    "            else:\n",
    "                damage = monster[1]\n",
    "        else:\n",
    "            population = None\n",
    "            damage = None\n",
    "        monster_data = {\n",
    "            \"name\": monster[0],\n",
    "            \"str\": monster[1],\n",
    "            \"lat\": lat,\n",
    "            \"long\": long,\n",
    "            \"ts\": str(datetime.now()),\n",
    "            \"country\": country,\n",
    "            \"population\": population,\n",
    "            \"damage\": damage\n",
    "        }\n",
    "        monster_json_string += json.dumps(monster_data)  \n",
    "    # print(monster_json_string)\n",
    "    monster_data = [(monster_json_string,),]\n",
    "    random_monster_df = spark.createDataFrame(monster_data, [\"value\"])\n",
    "    #random_monster_df.show(1, False)\n",
    "    random_monster_df.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"monster_movements\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3ea579-d5e8-4df3-a758-08247ef84518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geonamescache in /opt/conda/lib/python3.11/site-packages (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66b7242-0aed-4e2d-bc50-c34cd6cfb200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
